{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de3735a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\wilma\\anaconda3\\Lib\\site-packages\\openpyxl\\worksheet\\_read_only.py:81: UserWarning: Data Validation extension is not supported and will be removed\n",
      "  for idx, row in parser.parse():\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from datetime import timedelta\n",
    "import math\n",
    "import copy\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "random.seed(20)\n",
    "\n",
    "# --- Data Loading ---\n",
    "file_path = \"generic_input_case.xlsx\" # Path to the input data\n",
    "\n",
    "# Load data from Excel sheets into DataFrames\n",
    "df_horizon = pd.read_excel(file_path, sheet_name=\"HORIZONTE\") # Planning horizon\n",
    "df_up_database = pd.read_excel(file_path, sheet_name=\"BD_UP\") # Production Unit details\n",
    "df_fleet = pd.read_excel(file_path, sheet_name=\"FROTA\")       # Transporter fleet info\n",
    "df_crane = pd.read_excel(file_path, sheet_name=\"GRUA\")       # Crane capacity/rules\n",
    "df_factory = pd.read_excel(file_path, sheet_name=\"FABRICA\")   # Factory constraints\n",
    "df_route = pd.read_excel(file_path, sheet_name=\"ROTA\")       # Route details (UP -> Transporter)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d48ee2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Constraint Validation Functions ---\n",
    "\n",
    "def validate_daily_demand(schedule_df, df_factory, flag):\n",
    "    \"\"\"Checks if daily delivered volume meets factory demand constraints.\"\"\"\n",
    "    daily_volume = schedule_df.groupby('DIA')['VOLUME'].sum()\n",
    "    merged = pd.merge(daily_volume.reset_index(), df_factory[['DIA', 'DEMANDA_MIN', 'DEMANDA_MAX']], on='DIA')\n",
    "\n",
    "    # Check for violations\n",
    "    violations = merged[\n",
    "        (merged['VOLUME'] < merged['DEMANDA_MIN']) |\n",
    "        (merged['VOLUME'] > merged['DEMANDA_MAX'])\n",
    "    ]\n",
    "\n",
    "    if not violations.empty:\n",
    "        if flag == 1: # Optionally print details\n",
    "            print(\"Violation - Daily Demand:\")\n",
    "            print(violations)\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def validate_daily_rsp(schedule_df, df_factory, flag):\n",
    "    \"\"\"Checks if the daily weighted average RSP is within factory limits.\"\"\"\n",
    "    if schedule_df.empty or 'VOLUME' not in schedule_df.columns or 'RSP' not in schedule_df.columns:\n",
    "        return True # Pass if no relevant data\n",
    "\n",
    "    # Calculate weighted average RSP per day\n",
    "    schedule_df['RSP_WEIGHTED_SUM'] = schedule_df['RSP'] * schedule_df['VOLUME']\n",
    "    daily_rsp_sum = schedule_df.groupby('DIA')['RSP_WEIGHTED_SUM'].sum()\n",
    "    daily_volume_sum = schedule_df.groupby('DIA')['VOLUME'].sum()\n",
    "    daily_avg_rsp = (daily_rsp_sum / daily_volume_sum).fillna(0).reset_index()\n",
    "    daily_avg_rsp.rename(columns={0: 'RSP_AVG'}, inplace=True)\n",
    "\n",
    "    # Compare against limits\n",
    "    merged = pd.merge(daily_avg_rsp, df_factory[['DIA', 'RSP_MIN', 'RSP_MAX']], on='DIA')\n",
    "    violations = merged[\n",
    "        (round(merged['RSP_AVG'], 2) < round(merged['RSP_MIN'], 2)) |\n",
    "        (round(merged['RSP_AVG'], 2) > round(merged['RSP_MAX'], 2))\n",
    "    ]\n",
    "\n",
    "    if not violations.empty:\n",
    "        if flag == 1: # Optionally print details\n",
    "            print(\"Violation - Daily RSP:\")\n",
    "            print(violations)\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def validate_transporter_fleet_size(schedule_df, df_fleet):\n",
    "    \"\"\"Checks if daily vehicle usage per transporter is within their fleet limits.\"\"\"\n",
    "    daily_fleet = schedule_df.groupby(['DIA', 'TRANSPORTADOR'])['QTD_VEICULOS'].sum().reset_index()\n",
    "    merged = pd.merge(daily_fleet, df_fleet[['TRANSPORTADOR', 'FROTA_MIN', 'FROTA_MAX']], on='TRANSPORTADOR')\n",
    "\n",
    "    # Check for violations\n",
    "    violations = merged[\n",
    "        (merged['QTD_VEICULOS'] < merged['FROTA_MIN']) |\n",
    "        (merged['QTD_VEICULOS'] > merged['FROTA_MAX'])\n",
    "    ]\n",
    "\n",
    "    if not violations.empty:\n",
    "        print(\"Violation - Transporter Fleet Size:\")\n",
    "        print(violations)\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def validate_crane_capacity(schedule_df, df_crane):\n",
    "    \"\"\"Checks if a transporter serves more UPs simultaneously than their crane capacity.\"\"\"\n",
    "    daily_up_count = schedule_df.groupby(['DIA', 'TRANSPORTADOR'])['UP'].nunique().reset_index()\n",
    "    daily_up_count.rename(columns={'UP': 'UP_COUNT'}, inplace=True)\n",
    "    merged = pd.merge(daily_up_count, df_crane[['TRANSPORTADOR', 'QTD_GRUAS']], on='TRANSPORTADOR')\n",
    "\n",
    "    # Check for violations\n",
    "    violations = merged[merged['UP_COUNT'] > merged['QTD_GRUAS']]\n",
    "\n",
    "    if not violations.empty:\n",
    "        print(\"Violation - Crane Capacity:\")\n",
    "        print(violations)\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def validate_fazenda_exclusivity(schedule_df):\n",
    "    \"\"\"Checks if a transporter operates in more than one Fazenda per day.\"\"\"\n",
    "    daily_fazenda_count = schedule_df.groupby(['DIA', 'TRANSPORTADOR'])['FAZENDA'].nunique().reset_index()\n",
    "    daily_fazenda_count.rename(columns={'FAZENDA': 'FAZENDA_COUNT'}, inplace=True)\n",
    "\n",
    "    # Check for violations\n",
    "    violations = daily_fazenda_count[daily_fazenda_count['FAZENDA_COUNT'] > 1]\n",
    "\n",
    "    if not violations.empty:\n",
    "        print(\"Violation - Fazenda Exclusivity:\")\n",
    "        print(violations)\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def validate_min_vehicles_per_up(schedule_df, df_crane):\n",
    "    \"\"\"Checks if the minimum vehicle percentage per UP assignment is met.\"\"\"\n",
    "    total_daily_fleet = schedule_df.groupby(['DIA', 'TRANSPORTADOR'])['QTD_VEICULOS'].sum().reset_index()\n",
    "    total_daily_fleet.rename(columns={'QTD_VEICULOS': 'TOTAL_VEHICULOS_DIA'}, inplace=True)\n",
    "\n",
    "    merged = pd.merge(schedule_df, total_daily_fleet, on=['DIA', 'TRANSPORTADOR'])\n",
    "    merged = pd.merge(merged, df_crane[['TRANSPORTADOR', 'PORCENTAGEM_VEICULOS_MIN']], on='TRANSPORTADOR')\n",
    "\n",
    "    # Calculate actual percentage, handling potential division by zero\n",
    "    merged['PORCENTAJE_REAL'] = np.where(merged['TOTAL_VEHICULOS_DIA'] > 0,\n",
    "                                         merged['QTD_VEICULOS'] / merged['TOTAL_VEHICULOS_DIA'],\n",
    "                                         0)\n",
    "\n",
    "    # Check for violations (using tolerance)\n",
    "    tolerance = 1e-6\n",
    "    violations = merged[merged['PORCENTAJE_REAL'] < merged['PORCENTAGEM_VEICULOS_MIN'] - tolerance]\n",
    "\n",
    "    if not violations.empty:\n",
    "        print(\"Violation - Minimum Vehicles per UP:\")\n",
    "        print(violations[['DIA', 'TRANSPORTADOR', 'UP', 'QTD_VEICULOS', 'TOTAL_VEHICULOS_DIA', 'PORCENTAGEM_VEICULOS_MIN', 'PORCENTAJE_REAL']])\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def check_continuity(days_list):\n",
    "    \"\"\"Helper to check work continuity and count work blocks for a list of days.\"\"\"\n",
    "    if not isinstance(days_list, list) or not days_list:\n",
    "         return 0, True # No days worked -> 0 blocks, continuous\n",
    "\n",
    "    # Filter NaNs, get unique days, sort\n",
    "    sorted_days = sorted(list(set(filter(lambda x: x is not None and not pd.isna(x), days_list))))\n",
    "    if not sorted_days:\n",
    "      return 0, True # No valid days -> 0 blocks, continuous\n",
    "\n",
    "    blocks = 1 # Start assuming one block\n",
    "    for i in range(len(sorted_days) - 1):\n",
    "        if sorted_days[i+1] != sorted_days[i] + 1: # Check for gap\n",
    "            blocks += 1\n",
    "    is_continuous = (blocks <= 1)\n",
    "    return blocks, is_continuous\n",
    "\n",
    "def validate_completion_rules(schedule_df, df_up_database):\n",
    "    \"\"\"Validates UP completion rules based on size (<7000m続 vs >=7000m続).\"\"\"\n",
    "    all_valid = True\n",
    "    total_volume_transported = schedule_df.groupby('UP')['VOLUME'].sum()\n",
    "\n",
    "    for up_id, group in schedule_df.groupby('UP'):\n",
    "        try:\n",
    "            up_info = df_up_database[df_up_database['UP'] == up_id].iloc[0]\n",
    "            total_volume_up = up_info['VOLUME']\n",
    "            volume_transported_up = total_volume_transported.get(up_id, 0)\n",
    "            days_worked = group['DIA'].tolist()\n",
    "            num_blocks, _ = check_continuity(days_worked) # Use check_continuity helper\n",
    "\n",
    "            is_small_up = total_volume_up < 7000\n",
    "\n",
    "            # Rule: Small UP (<7000 m続) - must be completed if started, in 1 block\n",
    "            if is_small_up:\n",
    "                if 0 < volume_transported_up < total_volume_up:\n",
    "                     print(f\"Violation UP < 7000: UP {up_id} incomplete. \"\n",
    "                           f\"Transported: {volume_transported_up:.2f}, Total: {total_volume_up:.2f}\")\n",
    "                     all_valid = False\n",
    "                if num_blocks > 1:\n",
    "                     print(f\"Violation UP < 7000: UP {up_id} has {num_blocks} blocks (max 1). \"\n",
    "                           f\"Days: {sorted(list(set(days_worked)))}\")\n",
    "                     all_valid = False\n",
    "\n",
    "            # Rule: Large UP (>=7000 m続) - max 2 blocks allowed\n",
    "            else:\n",
    "                if num_blocks > 2:\n",
    "                     print(f\"Violation UP >= 7000: UP {up_id} has {num_blocks} blocks (max 2). \"\n",
    "                           f\"Days: {sorted(list(set(days_worked)))}\")\n",
    "                     all_valid = False\n",
    "        except IndexError:\n",
    "            print(f\"Warning: Missing info for UP {up_id}. Skipping completion check.\")\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            print(f\"Error validating completion for UP {up_id}: {e}\")\n",
    "            all_valid = False\n",
    "\n",
    "    # Note: Fazenda completion rules are more complex and not fully validated here.\n",
    "    return all_valid\n",
    "\n",
    "# --- Main Validation Wrappers ---\n",
    "\n",
    "def validate_essential_constraints(schedule_df, df_factory, df_fleet, df_crane, df_up_database, flag):\n",
    "    \"\"\"Runs essential validations (demand, RSP, fleet, crane, exclusivity).\"\"\"\n",
    "    is_feasible = True\n",
    "    temp_schedule = schedule_df.copy() # Work on a copy to avoid modifying original\n",
    "\n",
    "    if 'RSP_WEIGHTED_SUM' in temp_schedule.columns:\n",
    "        temp_schedule.drop(columns=['RSP_WEIGHTED_SUM'], inplace=True, errors='ignore')\n",
    "\n",
    "    if not validate_daily_demand(temp_schedule, df_factory, flag): is_feasible = False\n",
    "    if not validate_daily_rsp(temp_schedule, df_factory, flag): is_feasible = False # Modifies temp_schedule\n",
    "    if not validate_transporter_fleet_size(temp_schedule, df_fleet): is_feasible = False\n",
    "    if not validate_crane_capacity(temp_schedule, df_crane): is_feasible = False\n",
    "    if not validate_fazenda_exclusivity(temp_schedule): is_feasible = False\n",
    "\n",
    "    return is_feasible\n",
    "\n",
    "# Full validation including completion rules (can be slower)\n",
    "# def validate_all_constraints(schedule_df, df_factory, df_fleet, df_crane, df_up_database, flag):\n",
    "#     \"\"\"Runs all validations including completion rules.\"\"\"\n",
    "#     print(\"--- Starting Full Constraint Validation ---\")\n",
    "#     is_feasible = validate_essential_constraints(schedule_df, df_factory, df_fleet, df_crane, df_up_database, flag)\n",
    "#\n",
    "#     if not validate_completion_rules(schedule_df, df_up_database): is_feasible = False\n",
    "#     # Add validate_min_vehicles_per_up if needed, ensuring schedule_df is comprehensive\n",
    "#     # try:\n",
    "#     #     if not validate_min_vehicles_per_up(schedule_df, df_crane): is_feasible = False\n",
    "#     # except Exception as e: is_feasible = False; print(f\"Min vehicles check failed: {e}\")\n",
    "#\n",
    "#     print(\"--- Validation Finished ---\")\n",
    "#     status = \"MEETS\" if is_feasible else \"DOES NOT MEET\"\n",
    "#     print(f\">>> Solution {status} all verified constraints.\")\n",
    "#     return is_feasible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7443c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Auxiliary & Objective Functions ---\n",
    "\n",
    "def calculate_daily_transport_volume(vehicles, load_capacity, cycle_time):\n",
    "    \"\"\"Calculates potential daily volume for a given assignment.\"\"\"\n",
    "    if vehicles <= 0 or load_capacity <= 0 or cycle_time <= 0:\n",
    "        return 0\n",
    "    return vehicles * load_capacity * cycle_time\n",
    "\n",
    "def calculate_objective(schedule_df):\n",
    "    \"\"\"Calculates the average daily DB variation (objective function).\"\"\"\n",
    "    if schedule_df is None or schedule_df.empty or 'DIA' not in schedule_df.columns or 'DB' not in schedule_df.columns:\n",
    "        return float('inf')\n",
    "\n",
    "    total_variation = 0\n",
    "    # Use unique days present in the schedule for averaging\n",
    "    days_in_schedule = schedule_df['DIA'].unique()\n",
    "    num_days_total = len(days_in_schedule) if len(days_in_schedule) > 0 else 1 # Avoid division by zero\n",
    "\n",
    "    # Calculate variation only for days with data\n",
    "    daily_stats = schedule_df.groupby('DIA')['DB'].agg(['min', 'max'])\n",
    "    daily_stats.dropna(inplace=True) # Ignore days with NaN DBs\n",
    "    total_variation = (daily_stats['max'] - daily_stats['min']).sum()\n",
    "\n",
    "    # Average over the number of unique days in the schedule\n",
    "    average_variation = total_variation / num_days_total\n",
    "    return average_variation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1909f31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Constructive Heuristic ---\n",
    "\n",
    "def constructive_heuristic(df_horizon, df_up_db_input, df_fleet, df_crane, df_factory, df_route, flag):\n",
    "    \"\"\"Builds an initial schedule day-by-day using greedy choices.\"\"\"\n",
    "    schedule_entries = [] # Use a more descriptive name\n",
    "\n",
    "    # Initialize UP status tracker\n",
    "    up_status = df_up_db_input.set_index('UP').copy()\n",
    "    up_status['VOLUME_RESTANTE'] = up_status['VOLUME']\n",
    "    up_status['DIAS_TRABAJADOS'] = [[] for _ in range(len(up_status))]\n",
    "    up_status['BLOQUES_TRABAJO'] = 0\n",
    "    up_status['ESTADO'] = 'PENDIENTE' # PENDING, ACTIVA, COMPLETADA, PAUSADA\n",
    "\n",
    "    # Track transporter state\n",
    "    transporter_last_fazenda = {t: None for t in df_fleet['TRANSPORTADOR']}\n",
    "    transporters = list(df_fleet['TRANSPORTADOR'].unique())\n",
    "    random.shuffle(transporters) # Randomize transporter order\n",
    "\n",
    "    up_status = up_status.sample(frac=1) # Randomize UP order\n",
    "\n",
    "    # --- Daily Loop ---\n",
    "    for _, day_info in df_horizon.iterrows():\n",
    "        current_day = day_info['DIA']\n",
    "        current_month = day_info['MES']\n",
    "\n",
    "        # Prioritize UPs: active/paused first, then pending\n",
    "        ups_active_paused = up_status[up_status['ESTADO'].isin(['ACTIVA', 'PAUSADA'])].index.tolist()\n",
    "        ups_pending = up_status[up_status['ESTADO'] == 'PENDIENTE'].index.tolist()\n",
    "        prioritized_ups = ups_active_paused + ups_pending\n",
    "\n",
    "        # --- Transporter Loop ---\n",
    "        for transporter in transporters:\n",
    "            try:\n",
    "                t_info = df_fleet[df_fleet['TRANSPORTADOR'] == transporter].iloc[0]\n",
    "                c_info = df_crane[df_crane['TRANSPORTADOR'] == transporter].iloc[0]\n",
    "                fleet_min, fleet_max = t_info['FROTA_MIN'], t_info['FROTA_MAX']\n",
    "                num_cranes = c_info['QTD_GRUAS']\n",
    "                min_veh_perc = c_info[\"PORCENTAGEM_VEICULOS_MIN\"]\n",
    "            except IndexError:\n",
    "                 continue\n",
    "\n",
    "            # --- Determine Target Fazenda (prioritize continuity) ---\n",
    "            target_fazenda = None\n",
    "            prev_fazenda = transporter_last_fazenda[transporter]\n",
    "            if prev_fazenda:\n",
    "                 ups_in_prev = up_status[up_status['FAZENDA'] == prev_fazenda]\n",
    "                 if any(ups_in_prev['ESTADO'].isin(['ACTIVA', 'PAUSADA'])):\n",
    "                     target_fazenda = prev_fazenda\n",
    "\n",
    "            # If no continuity needed, find a new Fazenda\n",
    "            if not target_fazenda:\n",
    "                 possible_fazendas = up_status[up_status.index.isin(prioritized_ups)]['FAZENDA'].unique()\n",
    "                 random.shuffle(possible_fazendas)\n",
    "                 for f in possible_fazendas:\n",
    "                     ups_in_f = up_status[(up_status['FAZENDA'] == f) & up_status.index.isin(prioritized_ups)].index\n",
    "                     # Check if transporter has a route to any prioritized UP in this fazenda\n",
    "                     if any(not df_route[(df_route['ORIGEM'] == up) & (df_route['TRANSPORTADOR'] == transporter)].empty for up in ups_in_f):\n",
    "                          target_fazenda = f\n",
    "                          break\n",
    "\n",
    "            if not target_fazenda:\n",
    "                transporter_last_fazenda[transporter] = None\n",
    "                continue # No suitable Fazenda found for this transporter today\n",
    "\n",
    "            transporter_last_fazenda[transporter] = target_fazenda # Update last worked\n",
    "\n",
    "            # --- Select UPs within the Target Fazenda (respecting crane limit) ---\n",
    "            selected_ups_today = []\n",
    "            candidate_ups = [up for up in prioritized_ups if up_status.loc[up, 'FAZENDA'] == target_fazenda]\n",
    "\n",
    "            for up_id in candidate_ups:\n",
    "                if len(selected_ups_today) >= num_cranes: break # Crane limit\n",
    "\n",
    "                # Check route exists\n",
    "                if df_route[(df_route['ORIGEM'] == up_id) & (df_route['TRANSPORTADOR'] == transporter)].empty:\n",
    "                    continue\n",
    "\n",
    "                # Check completion rules feasibility (simplified check)\n",
    "                try:\n",
    "                    up_data = up_status.loc[up_id]\n",
    "                    if up_data['VOLUME_RESTANTE'] <= 1e-6: continue # Already done\n",
    "\n",
    "                    can_process = False\n",
    "                    state = up_data['ESTADO']\n",
    "                    blocks = up_data['BLOQUES_TRABAJO']\n",
    "                    is_small = up_data['VOLUME'] < 7000\n",
    "\n",
    "                    if state == 'PENDIENTE':\n",
    "                        can_process = True if is_small else (blocks < 2)\n",
    "                    elif state == 'ACTIVA':\n",
    "                        can_process = True\n",
    "                    elif state == 'PAUSADA':\n",
    "                        can_process = (blocks < 2)\n",
    "\n",
    "                    if can_process:\n",
    "                        selected_ups_today.append(up_id)\n",
    "                except KeyError: continue # Skip if UP data missing\n",
    "\n",
    "            if not selected_ups_today: continue # No UPs selected\n",
    "\n",
    "            # --- Assign Vehicles ---\n",
    "            num_ups = len(selected_ups_today)\n",
    "            vehicles_per_up = {}\n",
    "            # Simple heuristic: assign max fleet, distribute somewhat evenly respecting min %\n",
    "            total_vehicles = fleet_max\n",
    "            min_req_each = math.ceil(min_veh_perc * total_vehicles)\n",
    "            base_assign = max(1, min_req_each)\n",
    "            remaining_veh = total_vehicles\n",
    "            temp_assignment = {}\n",
    "\n",
    "            # Assign base minimum\n",
    "            for up_id in selected_ups_today:\n",
    "                 assign_count = min(base_assign, remaining_veh)\n",
    "                 if assign_count > 0:\n",
    "                     temp_assignment[up_id] = assign_count\n",
    "                     remaining_veh -= assign_count\n",
    "                 else: temp_assignment[up_id] = 0\n",
    "\n",
    "            # Distribute remainder\n",
    "            up_order = list(selected_ups_today)\n",
    "            random.shuffle(up_order)\n",
    "            idx = 0\n",
    "            while remaining_veh > 0 and num_ups > 0:\n",
    "                 up_to_inc = up_order[idx % num_ups]\n",
    "                 temp_assignment[up_to_inc] += 1\n",
    "                 remaining_veh -= 1\n",
    "                 idx += 1\n",
    "\n",
    "            # Final check: ensure assigned vehicles can transport volume & respect fleet total\n",
    "            final_veh_assigned_today = 0\n",
    "            for up_id in selected_ups_today:\n",
    "                assigned_veh = temp_assignment.get(up_id, 0)\n",
    "                if assigned_veh <= 0: continue\n",
    "                try:\n",
    "                    route_info = df_route[(df_route['ORIGEM'] == up_id) & (df_route['TRANSPORTADOR'] == transporter)].iloc[0]\n",
    "                    vol = calculate_daily_transport_volume(assigned_veh, route_info['CAIXA_CARGA'], route_info['TEMPO_CICLO'])\n",
    "                    if vol > 1e-6:\n",
    "                        vehicles_per_up[up_id] = assigned_veh\n",
    "                        final_veh_assigned_today += assigned_veh\n",
    "                except IndexError: continue # Skip if no route info\n",
    "\n",
    "            # Recalculate selected UPs based on valid assignments\n",
    "            selected_ups_today = list(vehicles_per_up.keys())\n",
    "\n",
    "            # --- Calculate Transported Volume & Update State ---\n",
    "            for up_id in selected_ups_today:\n",
    "                assigned_vehicles = vehicles_per_up[up_id]\n",
    "                try:\n",
    "                    up_data = up_status.loc[up_id]\n",
    "                    route_info = df_route[(df_route['ORIGEM'] == up_id) & (df_route['TRANSPORTADOR'] == transporter)].iloc[0]\n",
    "                    max_vol_today = calculate_daily_transport_volume(assigned_vehicles, route_info['CAIXA_CARGA'], route_info['TEMPO_CICLO'])\n",
    "                    transported_vol = min(max_vol_today, up_data['VOLUME_RESTANTE'])\n",
    "\n",
    "                    if transported_vol > 1e-6:\n",
    "                        # Update UP state\n",
    "                        up_status.loc[up_id, 'VOLUME_RESTANTE'] -= transported_vol\n",
    "                        if up_status.loc[up_id, 'VOLUME_RESTANTE'] < 0: up_status.loc[up_id, 'VOLUME_RESTANTE'] = 0\n",
    "\n",
    "                        # Update work days and blocks safely\n",
    "                        days_worked = up_status.at[up_id, 'DIAS_TRABAJADOS']\n",
    "                        if not isinstance(days_worked, list): days_worked = []\n",
    "                        last_day = days_worked[-1] if days_worked else None\n",
    "                        is_new_block = (last_day is None) or (current_day != last_day + 1)\n",
    "\n",
    "                        if is_new_block and last_day is not None: # Increment block count if gap\n",
    "                            if pd.isna(up_status.loc[up_id, 'BLOQUES_TRABAJO']): up_status.loc[up_id, 'BLOQUES_TRABAJO'] = 0\n",
    "                            up_status.loc[up_id, 'BLOQUES_TRABAJO'] += 1\n",
    "\n",
    "                        if current_day not in days_worked: # Add current day\n",
    "                            days_worked.append(current_day)\n",
    "                            up_status.at[up_id, 'DIAS_TRABAJADOS'] = days_worked # Assign back\n",
    "\n",
    "                        # Set final state for the day\n",
    "                        new_state = 'COMPLETADA' if up_status.loc[up_id, 'VOLUME_RESTANTE'] <= 1e-6 else 'ACTIVA'\n",
    "                        up_status.loc[up_id, 'ESTADO'] = new_state\n",
    "\n",
    "                        # Record the schedule entry\n",
    "                        schedule_entries.append({\n",
    "                            'UP': up_id, 'FAZENDA': up_data['FAZENDA'], 'TRANSPORTADOR': transporter,\n",
    "                            'DIA': current_day, 'MES': current_month, 'DB': up_data['DB'],\n",
    "                            'RSP': up_data['RSP'], 'QTD_VEICULOS': assigned_vehicles,\n",
    "                            'VOLUME': transported_vol\n",
    "                        })\n",
    "                        accumulated_volume_day += transported_vol\n",
    "\n",
    "                except (IndexError, KeyError) as e:\n",
    "                     # print(f\"Warn: Data missing for {up_id}/{transporter}, day {current_day}. Skipping entry. Error: {e}\")\n",
    "                     continue\n",
    "                except Exception as e:\n",
    "                     print(f\"Error processing UP {up_id}, Day {current_day}: {e}\")\n",
    "                     continue\n",
    "\n",
    "    # --- End of Horizon ---\n",
    "    final_schedule = pd.DataFrame(schedule_entries)\n",
    "    return final_schedule\n",
    "\n",
    "# --- Pattern-Based Heuristic Components ---\n",
    "\n",
    "def validate_single_day(schedule_df_day, df_factory_day, df_fleet, df_crane):\n",
    "    \"\"\"Validates essential constraints for a single day's schedule slice.\"\"\"\n",
    "    # Handle empty schedule case\n",
    "    if schedule_df_day is None or schedule_df_day.empty:\n",
    "        if df_factory_day is None or df_factory_day.empty or 'DEMANDA_MIN' not in df_factory_day.columns:\n",
    "            return False # Cannot validate without factory data\n",
    "        # Empty schedule is feasible only if min demand is zero\n",
    "        return df_factory_day['DEMANDA_MIN'].iloc[0] <= 1e-6\n",
    "\n",
    "    # Basic checks on input dataframes\n",
    "    if 'DIA' not in schedule_df_day.columns: return False\n",
    "    if not isinstance(df_factory_day, pd.DataFrame) or df_factory_day.empty: return False\n",
    "\n",
    "    try: # Ensure day value is usable\n",
    "        day = int(schedule_df_day['DIA'].iloc[0])\n",
    "    except: return False\n",
    "\n",
    "    # Filter factory constraints (should already be filtered, but double-check)\n",
    "    factory_constraints = df_factory_day[df_factory_day['DIA'] == day]\n",
    "    if factory_constraints.empty: return False\n",
    "\n",
    "    # Run essential validations (suppress internal prints with flag=0)\n",
    "    is_feasible = True\n",
    "    try:\n",
    "        # Check functions exist (optional, assumes script structure)\n",
    "        # required_funcs = ['validate_daily_demand', 'validate_daily_rsp', ...]\n",
    "        # if not all(...): return False\n",
    "\n",
    "        temp_schedule = schedule_df_day.copy() # Validate on a copy\n",
    "        if not validate_daily_demand(temp_schedule, factory_constraints, 0): is_feasible = False\n",
    "        if not validate_daily_rsp(temp_schedule, factory_constraints, 0): is_feasible = False\n",
    "        if not validate_transporter_fleet_size(temp_schedule, df_fleet): is_feasible = False\n",
    "        if not validate_crane_capacity(temp_schedule, df_crane): is_feasible = False\n",
    "        if not validate_fazenda_exclusivity(temp_schedule): is_feasible = False\n",
    "    except Exception as e:\n",
    "        print(f\"Error during single day validation (Day {day}): {e}\")\n",
    "        return False\n",
    "\n",
    "    return is_feasible\n",
    "\n",
    "def find_feasible_daily_pattern(target_day, current_up_status, df_horizon, df_fleet, df_crane, df_factory, df_route, max_attempts=100):\n",
    "    \"\"\"Tries to find a feasible schedule pattern for a target day.\"\"\"\n",
    "    # Prepare single-day inputs for the constructive heuristic\n",
    "    horizon_day_info = df_horizon[df_horizon['DIA'] == target_day]\n",
    "    if horizon_day_info.empty: return None, False\n",
    "    month_val = horizon_day_info['MES'].iloc[0]\n",
    "    year_val = horizon_day_info.get('ANO', pd.Series([None])).iloc[0] # Handle missing ANO\n",
    "    df_horizon_day = pd.DataFrame([{'DIA': target_day, 'MES': month_val, 'ANO': year_val}])\n",
    "    df_factory_day = df_factory[df_factory['DIA'] == target_day].copy()\n",
    "    if df_factory_day.empty: return None, False\n",
    "\n",
    "    last_attempt_schedule = None\n",
    "    for attempt in range(max_attempts):\n",
    "        # Use a copy of the current state for the heuristic run\n",
    "        up_status_for_heuristic = current_up_status.copy().reset_index()\n",
    "        daily_schedule_attempt = None\n",
    "\n",
    "        try:\n",
    "            # Check if heuristic function exists\n",
    "            if 'constructive_heuristic' not in globals() or not callable(globals()['constructive_heuristic']):\n",
    "                 print(\"FATAL: `constructive_heuristic` not defined.\"); return None, False\n",
    "\n",
    "            # Run heuristic for the single day\n",
    "            daily_schedule_attempt = constructive_heuristic(\n",
    "                df_horizon_day, up_status_for_heuristic, df_fleet, df_crane,\n",
    "                df_factory_day, df_route, flag=0\n",
    "            )\n",
    "            last_attempt_schedule = daily_schedule_attempt # Keep track of last try\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error in constructive_heuristic (Day {target_day}, Att {attempt+1}): {e}\")\n",
    "            continue # Try again\n",
    "\n",
    "        # Filter result for the target day (should be redundant but safe)\n",
    "        if daily_schedule_attempt is not None and not daily_schedule_attempt.empty:\n",
    "             if 'DIA' in daily_schedule_attempt.columns:\n",
    "                 daily_schedule_attempt = daily_schedule_attempt[daily_schedule_attempt['DIA'] == target_day].copy()\n",
    "             else: daily_schedule_attempt = pd.DataFrame() # Treat as empty if DIA missing\n",
    "\n",
    "        # Validate this attempt\n",
    "        if validate_single_day(daily_schedule_attempt, df_factory_day, df_fleet, df_crane):\n",
    "            return daily_schedule_attempt, True # Return first feasible pattern\n",
    "\n",
    "    # If no feasible pattern found after max attempts\n",
    "    print(f\"Warn: No feasible pattern found for Day {target_day} after {max_attempts} attempts. Using last.\")\n",
    "    is_last_feasible = validate_single_day(last_attempt_schedule, df_factory_day, df_fleet, df_crane)\n",
    "    return last_attempt_schedule, is_last_feasible # Return last attempt and its feasibility\n",
    "\n",
    "\n",
    "def calculate_extension_days(daily_pattern, current_up_status, horizon_end_day, current_day):\n",
    "    \"\"\"Calculates how many days a feasible pattern can be repeated based on UP volume.\"\"\"\n",
    "    if daily_pattern is None or daily_pattern.empty:\n",
    "        # Extend empty pattern by 1 day (if within horizon)\n",
    "        return min(1, max(0, horizon_end_day - current_day + 1))\n",
    "\n",
    "    if 'UP' not in daily_pattern.columns or 'VOLUME' not in daily_pattern.columns:\n",
    "        print(\"Error: Pattern missing UP/VOLUME columns in calculate_extension_days.\")\n",
    "        return 0\n",
    "\n",
    "    volume_per_up = daily_pattern.groupby('UP')['VOLUME'].sum()\n",
    "    if volume_per_up.sum() <= 1e-6: # Pattern transports negligible volume\n",
    "        return min(1, max(0, horizon_end_day - current_day + 1))\n",
    "\n",
    "    max_k = float('inf')\n",
    "    for up_id, daily_vol in volume_per_up.items():\n",
    "        if daily_vol <= 1e-6: continue\n",
    "        if up_id not in current_up_status.index: return 0 # Required UP missing\n",
    "\n",
    "        remaining_vol = current_up_status.loc[up_id, 'VOLUME_RESTANTE']\n",
    "        if remaining_vol <= 1e-6: return 0 # Required UP is empty\n",
    "\n",
    "        days_possible = math.floor(remaining_vol / daily_vol)\n",
    "        max_k = min(max_k, days_possible)\n",
    "\n",
    "    days_left_horizon = max(0, horizon_end_day - current_day + 1)\n",
    "    final_k = min(max_k, days_left_horizon)\n",
    "\n",
    "    # Ensure we can apply at least once if some volume remains\n",
    "    if final_k == 0 and volume_per_up.sum() > 1e-6:\n",
    "        # Check if *any* UP required by the pattern still has volume\n",
    "        can_apply_once = any(current_up_status.loc[up, 'VOLUME_RESTANTE'] > 1e-6\n",
    "                             for up in volume_per_up[volume_per_up > 1e-6].index\n",
    "                             if up in current_up_status.index)\n",
    "        if can_apply_once:\n",
    "            final_k = 1\n",
    "\n",
    "    return max(0, int(final_k)) # Return non-negative integer\n",
    "\n",
    "\n",
    "# --- Main Pattern-Based Heuristic Loop ---\n",
    "def run_pattern_based_heuristic(df_horizon, df_up_database, df_fleet, df_crane, df_factory, df_route, max_daily_attempts=100):\n",
    "    \"\"\"Orchestrates the pattern-finding and extension heuristic.\"\"\"\n",
    "    print(\"--- Running Pattern-Based Heuristic ---\")\n",
    "    all_schedule_entries = []\n",
    "\n",
    "    # Input checks\n",
    "    if 'DIA' not in df_horizon.columns or df_horizon.empty: return None\n",
    "    if 'UP' not in df_up_database.columns or 'VOLUME' not in df_up_database.columns: return None\n",
    "    current_day = df_horizon['DIA'].min()\n",
    "    horizon_end_day = df_horizon['DIA'].max()\n",
    "\n",
    "    # Initialize UP status\n",
    "    up_status = df_up_database.set_index('UP').copy()\n",
    "    up_status['VOLUME_RESTANTE'] = up_status['VOLUME']\n",
    "    up_status['DIAS_TRABAJADOS'] = [[] for _ in range(len(up_status))]\n",
    "    up_status['BLOQUES_TRABAJO'] = 0\n",
    "    up_status['ESTADO'] = 'PENDIENTE'\n",
    "\n",
    "    while current_day <= horizon_end_day:\n",
    "        print(f\"\\nProcessing Day {current_day}...\")\n",
    "\n",
    "        # 1. Find daily pattern (feasible or best-effort)\n",
    "        daily_pattern, is_feasible = find_feasible_daily_pattern(\n",
    "            current_day, up_status, df_horizon, df_fleet, df_crane, df_factory, df_route, max_daily_attempts\n",
    "        )\n",
    "\n",
    "        if daily_pattern is None:\n",
    "             print(f\"Error: Heuristic failed completely for Day {current_day}. Skipping.\")\n",
    "             current_day += 1; continue\n",
    "\n",
    "        # 2. Calculate extension days (k)\n",
    "        if is_feasible:\n",
    "             k = calculate_extension_days(daily_pattern, up_status, horizon_end_day, current_day)\n",
    "             if k == 0:\n",
    "                 print(f\"Warn: Feasible pattern Day {current_day} cannot extend (k=0). Applying once.\")\n",
    "                 k = 1\n",
    "             else: print(f\"Feasible pattern Day {current_day}. Extending {k} day(s).\")\n",
    "        else:\n",
    "             print(f\"Warn: Using infeasible pattern for Day {current_day}. Applying once.\")\n",
    "             k = 1 # Apply infeasible pattern only once\n",
    "\n",
    "        # Ensure k is valid and within horizon\n",
    "        k = min(k, horizon_end_day - current_day + 1)\n",
    "        k = max(0, k)\n",
    "        if k == 0:\n",
    "            print(f\"Info: k=0 for Day {current_day}. Skipping application.\")\n",
    "            current_day += 1; continue\n",
    "\n",
    "        # 3. Apply pattern for k days and update state\n",
    "        for i in range(k):\n",
    "            day_to_apply = current_day + i\n",
    "            if day_to_apply > horizon_end_day: break\n",
    "            if not isinstance(daily_pattern, pd.DataFrame): break # Safety check\n",
    "\n",
    "            pattern_copy = daily_pattern.copy()\n",
    "            if not pattern_copy.empty:\n",
    "                 # Update day/month/year\n",
    "                 pattern_copy['DIA'] = day_to_apply\n",
    "                 h_info = df_horizon[df_horizon['DIA'] == day_to_apply]\n",
    "                 if not h_info.empty:\n",
    "                      pattern_copy['MES'] = h_info['MES'].iloc[0]\n",
    "                      if 'ANO' in h_info.columns: pattern_copy['ANO'] = h_info['ANO'].iloc[0]\n",
    "                 all_schedule_entries.append(pattern_copy)\n",
    "\n",
    "                 # Update UP state\n",
    "                 if 'UP' not in pattern_copy.columns or 'VOLUME' not in pattern_copy.columns: continue\n",
    "                 vol_today = pattern_copy.groupby('UP')['VOLUME'].sum()\n",
    "                 for up_id, vol in vol_today.items():\n",
    "                     if up_id in up_status.index and vol > 1e-6:\n",
    "                         try:\n",
    "                             up_status.loc[up_id, 'VOLUME_RESTANTE'] -= vol\n",
    "                             if up_status.loc[up_id, 'VOLUME_RESTANTE'] < 0: up_status.loc[up_id, 'VOLUME_RESTANTE'] = 0\n",
    "\n",
    "                             # Update work days/blocks\n",
    "                             days_list = up_status.at[up_id, 'DIAS_TRABAJADOS']\n",
    "                             if not isinstance(days_list, list): days_list = []\n",
    "                             last_day = days_list[-1] if days_list else None\n",
    "                             is_new_block = (last_day is None) or (day_to_apply != last_day + 1)\n",
    "                             if is_new_block and last_day is not None:\n",
    "                                 if pd.isna(up_status.at[up_id, 'BLOQUES_TRABAJO']): up_status.at[up_id, 'BLOQUES_TRABAJO'] = 0\n",
    "                                 up_status.at[up_id, 'BLOQUES_TRABAJO'] += 1\n",
    "                             if day_to_apply not in days_list:\n",
    "                                  days_list.append(day_to_apply)\n",
    "                                  up_status.at[up_id, 'DIAS_TRABAJADOS'] = days_list\n",
    "\n",
    "                             # Update status\n",
    "                             new_state = 'COMPLETADA' if up_status.loc[up_id, 'VOLUME_RESTANTE'] <= 1e-6 else 'ACTIVA'\n",
    "                             up_status.loc[up_id, 'ESTADO'] = new_state\n",
    "                         except Exception as e: print(f\"Error updating state {up_id} day {day_to_apply}: {e}\")\n",
    "\n",
    "        # 4. Advance current day\n",
    "        current_day += k # Advance by the number of days applied\n",
    "\n",
    "    print(\"\\n--- Pattern-Based Heuristic Finished ---\")\n",
    "    if not all_schedule_entries: return pd.DataFrame()\n",
    "    try: final_schedule = pd.concat(all_schedule_entries, ignore_index=True)\n",
    "    except Exception as e: print(f\"Error concatenating schedule: {e}\"); return None\n",
    "\n",
    "    # Final validation check\n",
    "    print(\"\\nValidating final generated schedule (essential constraints)...\")\n",
    "    try:\n",
    "        if 'validate_essential_constraints' in globals() and callable(globals()['validate_essential_constraints']):\n",
    "            is_final_feasible = validate_essential_constraints(final_schedule, df_factory, df_fleet, df_crane, df_up_database, flag=0)\n",
    "            status = \"Feasible\" if is_final_feasible else \"INFEASIBLE\"\n",
    "            print(f\">>> Final Schedule Status: {status} <<<\")\n",
    "        else: print(\"Warn: Cannot perform final validation.\")\n",
    "    except Exception as e: print(f\"Error during final validation: {e}\")\n",
    "\n",
    "    return final_schedule\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46b539e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Running Pattern-Based Heuristic ---\n",
      "\n",
      "Processing Day 1...\n",
      "Error processing UP S6C421, Day 1: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S5AW10, Day 1: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S5AW05, Day 1: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S5AW10, Day 1: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S5AW05, Day 1: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S5AK08, Day 1: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S5AK05, Day 1: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S6C335, Day 1: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S6C298, Day 1: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S3AX02, Day 1: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S3AX04, Day 1: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S6C421, Day 1: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S5AW05, Day 1: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S5AW14, Day 1: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S3AX02, Day 1: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S3AX01, Day 1: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Feasible pattern Day 1. Extending 1 day(s).\n",
      "\n",
      "Processing Day 2...\n",
      "Error processing UP S6BX01, Day 2: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S5AK08, Day 2: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S5AK09, Day 2: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S5AK08, Day 2: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S5AK09, Day 2: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Feasible pattern Day 2. Extending 5 day(s).\n",
      "\n",
      "Processing Day 7...\n",
      "Error processing UP S5AK09, Day 7: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S5AK05, Day 7: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S6C334, Day 7: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S6C297, Day 7: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S3AX04, Day 7: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S3AX06, Day 7: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Feasible pattern Day 7. Extending 1 day(s).\n",
      "\n",
      "Processing Day 8...\n",
      "Error processing UP S6BG14, Day 8: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S6BG13, Day 8: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S5AW10, Day 8: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S5AW14, Day 8: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S3AX04, Day 8: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S3AX03, Day 8: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Feasible pattern Day 8. Extending 1 day(s).\n",
      "\n",
      "Processing Day 9...\n",
      "Error processing UP S6C335, Day 9: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S6C298, Day 9: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S3AX01, Day 9: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S3AX04, Day 9: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S5AW13, Day 9: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S5AW14, Day 9: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S5AW13, Day 9: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S5AW05, Day 9: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S5AW13, Day 9: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S5AW05, Day 9: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S6C421, Day 9: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S6BX01, Day 9: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S3AX02, Day 9: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S3AX04, Day 9: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S5AK08, Day 9: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S5AK10, Day 9: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Warn: Feasible pattern Day 9 cannot extend (k=0). Applying once.\n",
      "\n",
      "Processing Day 10...\n",
      "Error processing UP S5AK05, Day 10: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S5AK09, Day 10: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S6C421, Day 10: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S5AW10, Day 10: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S5AW13, Day 10: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Feasible pattern Day 10. Extending 1 day(s).\n",
      "\n",
      "Processing Day 11...\n",
      "Error processing UP S5AW09, Day 11: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S5AW13, Day 11: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S5AW09, Day 11: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S5AW13, Day 11: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S6C421, Day 11: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S5AK10, Day 11: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S5AK05, Day 11: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S6C298, Day 11: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S6C335, Day 11: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S3AX02, Day 11: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S3AX04, Day 11: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S3AX03, Day 11: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S3AX01, Day 11: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S6BX01, Day 11: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S5AK05, Day 11: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S5AK09, Day 11: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Feasible pattern Day 11. Extending 1 day(s).\n",
      "\n",
      "Processing Day 12...\n",
      "Error processing UP S6C297, Day 12: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S6C298, Day 12: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S5AW05, Day 12: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S5AW13, Day 12: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S3AX02, Day 12: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S3AX04, Day 12: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Warn: Feasible pattern Day 12 cannot extend (k=0). Applying once.\n",
      "\n",
      "Processing Day 13...\n",
      "Error processing UP S6BX01, Day 13: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S5AW09, Day 13: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S5AW14, Day 13: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S5AK10, Day 13: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S5AK09, Day 13: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Warn: Feasible pattern Day 13 cannot extend (k=0). Applying once.\n",
      "\n",
      "Processing Day 14...\n",
      "Error processing UP S3AX03, Day 14: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S3AX02, Day 14: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S5AW10, Day 14: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S5AW14, Day 14: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S6BX01, Day 14: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Warn: Feasible pattern Day 14 cannot extend (k=0). Applying once.\n",
      "\n",
      "Processing Day 15...\n",
      "Error processing UP S3AX02, Day 15: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S3AX04, Day 15: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S5AK09, Day 15: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S5AK05, Day 15: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S6BX01, Day 15: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Warn: Feasible pattern Day 15 cannot extend (k=0). Applying once.\n",
      "\n",
      "Processing Day 16...\n",
      "Error processing UP S5AK10, Day 16: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S5AK05, Day 16: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S6C334, Day 16: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S6C298, Day 16: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S3AX03, Day 16: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S3AX01, Day 16: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Warn: Feasible pattern Day 16 cannot extend (k=0). Applying once.\n",
      "\n",
      "Processing Day 17...\n",
      "Error processing UP S6BG10, Day 17: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S6BG13, Day 17: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S5AK05, Day 17: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S5AK10, Day 17: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S3AX01, Day 17: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S3AX02, Day 17: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S6BG11, Day 17: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S6BG10, Day 17: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S3AX01, Day 17: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S3AX02, Day 17: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S5AW10, Day 17: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S5AW09, Day 17: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S6C421, Day 17: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S3AX04, Day 17: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S3AX01, Day 17: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S3AX01, Day 17: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S3AX03, Day 17: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Warn: Feasible pattern Day 17 cannot extend (k=0). Applying once.\n",
      "\n",
      "Processing Day 18...\n",
      "Error processing UP S6C421, Day 18: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S5AW13, Day 18: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S5AW10, Day 18: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S5AK10, Day 18: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S5AK09, Day 18: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Warn: Feasible pattern Day 18 cannot extend (k=0). Applying once.\n",
      "\n",
      "Processing Day 19...\n",
      "Error processing UP S6C421, Day 19: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S3AX03, Day 19: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S3AX06, Day 19: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S5AK08, Day 19: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S5AK09, Day 19: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Warn: Feasible pattern Day 19 cannot extend (k=0). Applying once.\n",
      "\n",
      "Processing Day 20...\n",
      "Error processing UP S3AX04, Day 20: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S3AX06, Day 20: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S5AK08, Day 20: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S5AK10, Day 20: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S6BX01, Day 20: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Warn: Feasible pattern Day 20 cannot extend (k=0). Applying once.\n",
      "\n",
      "Processing Day 21...\n",
      "Error processing UP S6BX01, Day 21: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S5AW14, Day 21: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S5AW10, Day 21: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S3AX06, Day 21: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S3AX01, Day 21: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Warn: Feasible pattern Day 21 cannot extend (k=0). Applying once.\n",
      "\n",
      "Processing Day 22...\n",
      "Error processing UP S6C297, Day 22: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S6C334, Day 22: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S5AK08, Day 22: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S5AK05, Day 22: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S3AX04, Day 22: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S3AX06, Day 22: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Warn: Feasible pattern Day 22 cannot extend (k=0). Applying once.\n",
      "\n",
      "Processing Day 23...\n",
      "Error processing UP S6BG08, Day 23: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S6BG11, Day 23: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S3AX03, Day 23: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S3AX02, Day 23: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S3AX03, Day 23: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S3AX02, Day 23: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Warn: Feasible pattern Day 23 cannot extend (k=0). Applying once.\n",
      "\n",
      "Processing Day 24...\n",
      "Error processing UP S6BG13, Day 24: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S6BG11, Day 24: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S3AX03, Day 24: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S3AX01, Day 24: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S5AK10, Day 24: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S5AK09, Day 24: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Warn: Feasible pattern Day 24 cannot extend (k=0). Applying once.\n",
      "\n",
      "Processing Day 25...\n",
      "Error processing UP S3AX03, Day 25: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S3AX04, Day 25: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S6BX01, Day 25: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S3AX03, Day 25: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S3AX01, Day 25: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Warn: Feasible pattern Day 25 cannot extend (k=0). Applying once.\n",
      "\n",
      "Processing Day 26...\n",
      "Error processing UP S3AX01, Day 26: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S3AX06, Day 26: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S3AX01, Day 26: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S3AX06, Day 26: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S6C421, Day 26: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S6C421, Day 26: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S5AW13, Day 26: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S5AW09, Day 26: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S5AW13, Day 26: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S5AW09, Day 26: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S5AW05, Day 26: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S5AW14, Day 26: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S6BX01, Day 26: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S5AW05, Day 26: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S5AW14, Day 26: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Warn: Feasible pattern Day 26 cannot extend (k=0). Applying once.\n",
      "\n",
      "Processing Day 27...\n",
      "Error processing UP S6BX01, Day 27: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S5AK10, Day 27: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S5AK05, Day 27: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S5AK10, Day 27: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S5AK05, Day 27: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Warn: Feasible pattern Day 27 cannot extend (k=0). Applying once.\n",
      "\n",
      "Processing Day 28...\n",
      "Error processing UP S3AX01, Day 28: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S3AX04, Day 28: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S6C297, Day 28: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S6C334, Day 28: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S5AK05, Day 28: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S5AK09, Day 28: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Warn: Feasible pattern Day 28 cannot extend (k=0). Applying once.\n",
      "\n",
      "Processing Day 29...\n",
      "Error processing UP S6C335, Day 29: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S6C297, Day 29: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S5AW14, Day 29: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S5AW13, Day 29: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S3AX04, Day 29: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S3AX01, Day 29: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Warn: Feasible pattern Day 29 cannot extend (k=0). Applying once.\n",
      "\n",
      "Processing Day 30...\n",
      "Error processing UP S3AX04, Day 30: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S3AX01, Day 30: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S3AX01, Day 30: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S3AX03, Day 30: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S6C298, Day 30: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S6C334, Day 30: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S5AW14, Day 30: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S5AW05, Day 30: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S5AW14, Day 30: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S5AW05, Day 30: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S6C298, Day 30: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S6C334, Day 30: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Feasible pattern Day 30. Extending 1 day(s).\n",
      "\n",
      "Processing Day 31...\n",
      "Error processing UP S6BG12, Day 31: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S6BG13, Day 31: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S3AX06, Day 31: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S3AX04, Day 31: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S5AW05, Day 31: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S5AW14, Day 31: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S5AW05, Day 31: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S5AW10, Day 31: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S5AK10, Day 31: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S5AK09, Day 31: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S6C298, Day 31: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Error processing UP S6C335, Day 31: cannot access local variable 'accumulated_volume_day' where it is not associated with a value\n",
      "Warn: Feasible pattern Day 31 cannot extend (k=0). Applying once.\n",
      "\n",
      "--- Pattern-Based Heuristic Finished ---\n",
      "\n",
      "Validating final generated schedule (essential constraints)...\n",
      ">>> Final Schedule Status: Feasible <<<\n",
      "\n",
      "--- Initial Solution Generated ---\n",
      "       UP                      FAZENDA TRANSPORTADOR  DIA  MES          DB  \\\n",
      "0  S6C421                      SIRIEMA         Tover    1    5  406.666302   \n",
      "1  S5AW05  PIRACEMA_GLEBA PULADOR - DX       Pastori    1    5  450.000000   \n",
      "2  S5AW14  PIRACEMA_GLEBA PULADOR - DX       Pastori    1    5  467.553351   \n",
      "3  S3AX02                   INDIANA II       Rampazo    1    5  475.082497   \n",
      "4  S3AX01                   INDIANA II       Rampazo    1    5  476.359312   \n",
      "\n",
      "        RSP  QTD_VEICULOS  VOLUME   ANO  \n",
      "0  1.289003            14  2310.0  2020  \n",
      "1  1.440000            14  1386.0  2020  \n",
      "2  1.378530            13  1287.0  2020  \n",
      "3  1.540128            11  1306.8  2020  \n",
      "4  1.546497            10  1188.0  2020  \n",
      "Total entries: 166\n",
      "\n",
      "Initial Objective (Avg Daily DB Var): 59.8638\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Run Heuristic ---\n",
    "initial_solution = run_pattern_based_heuristic(\n",
    "    df_horizon, df_up_database, df_fleet, df_crane, df_factory, df_route, max_daily_attempts=200\n",
    ")\n",
    "\n",
    "if initial_solution is not None:\n",
    "    print(\"\\n--- Initial Solution Generated ---\")\n",
    "    print(initial_solution.head())\n",
    "    print(f\"Total entries: {len(initial_solution)}\")\n",
    "else:\n",
    "    print(\"\\n--- Heuristic failed to generate initial solution ---\")\n",
    "\n",
    "# Calculate initial objective\n",
    "initial_objective = calculate_objective(initial_solution) if initial_solution is not None else float('inf')\n",
    "print(f\"\\nInitial Objective (Avg Daily DB Var): {initial_objective:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85e5d08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Neighborhood Search (Simulated Annealing) ---\n",
    "\n",
    "def move_change_vehicles(current_schedule, df_route, df_fleet, df_up_database):\n",
    "    \"\"\"Neighborhood Move: Randomly change vehicle count for one task.\"\"\"\n",
    "    if current_schedule.empty: return current_schedule.copy()\n",
    "    neighbor = current_schedule.copy()\n",
    "    try: task_idx = random.choice(neighbor.index)\n",
    "    except IndexError: return neighbor\n",
    "\n",
    "    task = neighbor.loc[task_idx]\n",
    "    req_cols = ['TRANSPORTADOR', 'DIA', 'UP', 'QTD_VEICULOS', 'VOLUME']\n",
    "    if not all(col in task and pd.notna(task[col]) for col in req_cols): return neighbor\n",
    "\n",
    "    transp, day, up, current_veh = task['TRANSPORTADOR'], task['DIA'], task['UP'], task['QTD_VEICULOS']\n",
    "\n",
    "    try:\n",
    "        f_info = df_fleet[df_fleet['TRANSPORTADOR'] == transp].iloc[0]\n",
    "        f_min, f_max = f_info['FROTA_MIN'], f_info['FROTA_MAX']\n",
    "    except IndexError: return neighbor # Skip if no fleet info\n",
    "\n",
    "    # Current total vehicles for transporter on this day\n",
    "    current_total = neighbor[(neighbor['DIA'] == day) & (neighbor['TRANSPORTADOR'] == transp)]['QTD_VEICULOS'].sum(skipna=True)\n",
    "\n",
    "    change = random.choice([-1, 1])\n",
    "    new_veh_task = max(1, current_veh + change) # Ensure at least 1 vehicle\n",
    "\n",
    "    # New hypothetical total\n",
    "    new_total = current_total - current_veh + new_veh_task\n",
    "\n",
    "    # Check transporter fleet limits\n",
    "    if not (f_min <= new_total <= f_max):\n",
    "        return neighbor # Move violates fleet limits\n",
    "\n",
    "    # Apply change\n",
    "    neighbor.loc[task_idx, 'QTD_VEICULOS'] = new_veh_task\n",
    "\n",
    "    # Volume is NOT recalculated here for simplicity. Validation handles implications.\n",
    "    return neighbor\n",
    "\n",
    "# Add more sophisticated move functions here (e.g., reassign transporter, shift day)\n",
    "\n",
    "def simulated_annealing(initial_schedule, df_factory, df_fleet, df_crane, df_up_database, df_route,\n",
    "                        initial_temp=100, cooling_rate=0.99, max_iterations=1000,\n",
    "                        max_attempts_per_temp=50):\n",
    "    \"\"\"Optimizes a schedule using Simulated Annealing.\"\"\"\n",
    "    print(\"\\n--- Starting Simulated Annealing ---\")\n",
    "\n",
    "    # Validate starting point\n",
    "    try:\n",
    "        if 'validate_essential_constraints' not in globals() or not callable(globals()['validate_essential_constraints']):\n",
    "             print(\"Error: SA needs `validate_essential_constraints`.\"); return initial_schedule\n",
    "        if not validate_essential_constraints(initial_schedule, df_factory, df_fleet, df_crane, df_up_database, flag=0):\n",
    "            print(\"Error: Initial schedule for SA is infeasible.\"); return initial_schedule\n",
    "    except Exception as e:\n",
    "        print(f\"Error validating initial schedule for SA: {e}\"); return initial_schedule\n",
    "\n",
    "    # Initialize SA state\n",
    "    current_sched = initial_schedule.copy()\n",
    "    current_obj = calculate_objective(current_sched)\n",
    "    best_sched = current_sched.copy()\n",
    "    best_obj = current_obj\n",
    "    temp = initial_temp\n",
    "\n",
    "    print(f\"Initial Objective: {current_obj:.4f}\")\n",
    "\n",
    "    # SA Loop\n",
    "    for i in range(max_iterations):\n",
    "        if temp < 1e-3: print(\"Temperature threshold reached.\"); break # Stop condition\n",
    "\n",
    "        accepted_in_iter = False\n",
    "        for _ in range(max_attempts_per_temp): # Attempts at current temperature\n",
    "            # 1. Generate Neighbor\n",
    "            try:\n",
    "                move_func = random.choice([move_change_vehicles]) # Add more moves later\n",
    "                neighbor_sched = move_func(current_sched, df_route, df_fleet, df_up_database)\n",
    "            except Exception as e: continue # Try again if neighbor generation fails\n",
    "\n",
    "            # 2. Check Feasibility\n",
    "            try:\n",
    "                is_feasible = validate_essential_constraints(neighbor_sched, df_factory, df_fleet, df_crane, df_up_database, flag=0)\n",
    "            except Exception as e: is_feasible = False # Treat validation error as infeasible\n",
    "\n",
    "            # 3. Evaluate & Accept (if feasible)\n",
    "            if is_feasible:\n",
    "                neighbor_obj = calculate_objective(neighbor_sched)\n",
    "                delta = neighbor_obj - current_obj\n",
    "                accept = False\n",
    "                if delta < 0: # Always accept improvement\n",
    "                    accept = True\n",
    "                elif temp > 1e-9: # Accept worsening move probabilistically\n",
    "                    try:\n",
    "                        prob = math.exp(-delta / temp)\n",
    "                        if random.random() < prob: accept = True\n",
    "                    except OverflowError: accept = False\n",
    "\n",
    "                # 4. Update state if accepted\n",
    "                if accept:\n",
    "                    current_sched = neighbor_sched.copy()\n",
    "                    current_obj = neighbor_obj\n",
    "                    accepted_in_iter = True\n",
    "                    if current_obj < best_obj: # Update best found\n",
    "                        best_sched = current_sched.copy()\n",
    "                        best_obj = current_obj\n",
    "                    break # Move to next temperature iteration\n",
    "\n",
    "        # 5. Cool down\n",
    "        temp *= cooling_rate\n",
    "\n",
    "        # Log progress\n",
    "        if (i + 1) % 100 == 0:\n",
    "             print(f\"  Iter {i+1}/{max_iterations}, Temp: {temp:.3f}, \"\n",
    "                   f\"Current: {current_obj:.4f}, Best: {best_obj:.4f}\")\n",
    "\n",
    "    print(f\"--- Simulated Annealing Finished ---\")\n",
    "    print(f\"Final Best Objective: {best_obj:.4f}\")\n",
    "    return best_sched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05dc405d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initial solution validation status: FEASIBLE. Proceeding accordingly.\n",
      "\n",
      "--- Starting Simulated Annealing ---\n",
      "Initial Objective: 59.8638\n",
      "  Iter 100/1000, Temp: 3.029, Current: 59.8638, Best: 59.8638\n",
      "  Iter 200/1000, Temp: 1.835, Current: 59.8638, Best: 59.8638\n",
      "  Iter 300/1000, Temp: 1.111, Current: 59.8638, Best: 59.8638\n",
      "  Iter 400/1000, Temp: 0.673, Current: 59.8638, Best: 59.8638\n",
      "  Iter 500/1000, Temp: 0.408, Current: 59.8638, Best: 59.8638\n",
      "  Iter 600/1000, Temp: 0.247, Current: 59.8638, Best: 59.8638\n",
      "  Iter 700/1000, Temp: 0.150, Current: 59.8638, Best: 59.8638\n",
      "  Iter 800/1000, Temp: 0.091, Current: 59.8638, Best: 59.8638\n",
      "  Iter 900/1000, Temp: 0.055, Current: 59.8638, Best: 59.8638\n",
      "  Iter 1000/1000, Temp: 0.033, Current: 59.8638, Best: 59.8638\n",
      "--- Simulated Annealing Finished ---\n",
      "Final Best Objective: 59.8638\n",
      "\n",
      "--- Final Resulting Schedule ---\n",
      "         UP                      FAZENDA TRANSPORTADOR  DIA  MES          DB  \\\n",
      "0    S6C421                      SIRIEMA         Tover    1    5  406.666302   \n",
      "1    S5AW05  PIRACEMA_GLEBA PULADOR - DX       Pastori    1    5  450.000000   \n",
      "2    S5AW14  PIRACEMA_GLEBA PULADOR - DX       Pastori    1    5  467.553351   \n",
      "3    S3AX02                   INDIANA II       Rampazo    1    5  475.082497   \n",
      "4    S3AX01                   INDIANA II       Rampazo    1    5  476.359312   \n",
      "..      ...                          ...           ...  ...  ...         ...   \n",
      "161  S5AW10  PIRACEMA_GLEBA PULADOR - DX       Rampazo   31    5  489.657814   \n",
      "162  S5AK10              TURVO III (LEX)       Pastori   31    5  494.987423   \n",
      "163  S5AK09              TURVO III (LEX)       Pastori   31    5  485.001740   \n",
      "164  S6C298                    FORTALEZA         Tover   31    5  475.592290   \n",
      "165  S6C335                    FORTALEZA         Tover   31    5  462.157496   \n",
      "\n",
      "          RSP  QTD_VEICULOS  VOLUME  \n",
      "0    1.289003            14  2310.0  \n",
      "1    1.440000            14  1386.0  \n",
      "2    1.378530            13  1287.0  \n",
      "3    1.540128            11  1306.8  \n",
      "4    1.546497            10  1188.0  \n",
      "..        ...           ...     ...  \n",
      "161  1.441440            10   990.0  \n",
      "162  1.443982            14  1293.6  \n",
      "163  1.513740            13  1201.2  \n",
      "164  1.622175             7   970.2  \n",
      "165  1.519874             7   970.2  \n",
      "\n",
      "[166 rows x 9 columns]\n",
      "\n",
      "Optimized schedule saved to 'optimized_schedule_output.xlsx'\n",
      "Initial schedule saved to 'initial_schedule_output.xlsx'\n",
      "\n",
      "Final Objective Value: 59.8638\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Main Execution & Optimization ---\n",
    "\n",
    "# Check feasibility of the initial solution before SA\n",
    "is_initial_feasible = False\n",
    "if initial_solution is not None and not initial_solution.empty:\n",
    "    try:\n",
    "        if 'validate_essential_constraints' in globals() and callable(globals()['validate_essential_constraints']):\n",
    "             is_initial_feasible = validate_essential_constraints(initial_solution, df_factory, df_fleet, df_crane, df_up_database, flag=0)\n",
    "             status = \"FEASIBLE\" if is_initial_feasible else \"INFEASIBLE\"\n",
    "             print(f\"\\nInitial solution validation status: {status}. Proceeding accordingly.\")\n",
    "        else: print(\"\\nWarn: Cannot validate initial solution feasibility for SA.\")\n",
    "    except Exception as e: print(f\"Error validating initial solution: {e}\")\n",
    "\n",
    "# Run SA only if starting point is feasible\n",
    "if is_initial_feasible:\n",
    "    optimized_solution = simulated_annealing(\n",
    "        initial_schedule=initial_solution,\n",
    "        df_factory=df_factory, df_fleet=df_fleet, df_crane=df_crane,\n",
    "        df_up_database=df_up_database, df_route=df_route,\n",
    "        initial_temp=5,           # Starting temperature\n",
    "        cooling_rate=0.995,       # Cooling factor\n",
    "        max_iterations=1000,      # Number of iterations\n",
    "        max_attempts_per_temp=100 # Attempts per temperature level\n",
    "    )\n",
    "else:\n",
    "    print(\"\\nSkipping SA optimization due to infeasible/missing initial solution.\")\n",
    "    optimized_solution = initial_solution if initial_solution is not None else pd.DataFrame()\n",
    "\n",
    "# --- Final Output & Save ---\n",
    "print(\"\\n--- Final Resulting Schedule ---\")\n",
    "if optimized_solution is not None and not optimized_solution.empty:\n",
    "    # Clean up temporary columns before saving/displaying final\n",
    "    cols_to_drop = ['RSP_WEIGHTED_SUM', 'ANO'] # Add others if created temporarily\n",
    "    final_output = optimized_solution.drop(columns=cols_to_drop, errors='ignore')\n",
    "\n",
    "    print(final_output)\n",
    "    try:\n",
    "        final_output.to_excel(\"optimized_schedule_output.xlsx\", index=False)\n",
    "        print(\"\\nOptimized schedule saved to 'optimized_schedule_output.xlsx'\")\n",
    "        # Also save initial solution for comparison (if it exists)\n",
    "        if initial_solution is not None and not initial_solution.empty:\n",
    "             initial_solution.drop(columns=cols_to_drop, errors='ignore').to_excel(\"initial_schedule_output.xlsx\", index=False)\n",
    "             print(\"Initial schedule saved to 'initial_schedule_output.xlsx'\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError saving output files: {e}\")\n",
    "\n",
    "    final_obj = calculate_objective(final_output)\n",
    "    print(f\"\\nFinal Objective Value: {final_obj:.4f}\")\n",
    "else:\n",
    "    print(\"No final schedule generated.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
